{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data-Twitter_Creation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMs7hQ4+f/OzZA/r/hQjEsV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LloTjXqy8r5j","outputId":"9b136a0e-bb66-4318-f287-0c45dd918362"},"source":["# install huggingface\n","!pip install transformers\n","!pip install emoji"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 28.4MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n","Collecting huggingface-hub==0.0.8\n","  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 50.8MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 40.2MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n","Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n","Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 25.1MB/s \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XUUvETE0uW7F"},"source":["# import HuggingFace models\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n","\n","from nltk.tokenize import TweetTokenizer\n","tokenizer = TweetTokenizer()\n","\n","# import csv to deal with .csv data files\n","import csv\n","\n","# import pandas to just be able to visualise our data files\n","import pandas as pd\n","\n","# classic shit\n","import numpy as np\n","import torch\n","\n","# for getting right format for data\n","from torch.utils.data import Dataset, DataLoader\n","\n","# for computing metrics\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","# import for preprocess the data\n","import random\n","import emoji\n","import string\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R76T-E5XuauU"},"source":["%env WANDB_DISABLED=true"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TnZF4nyZuvyZ"},"source":["# Fetch the dataset from github and shuffle the rows. This because all the label 1 are in the same part of the dataset."]},{"cell_type":"code","metadata":{"id":"VxMFKeiZufFa"},"source":["!git clone https://github.com/MeliLuca/Natural_Language_Processing"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcddIf0SupBU"},"source":["# open train data\n","with open(\"/content/Natural_Language_Processing/sexism-dataset.csv\", \"r\", encoding=\"utf8\") as f:\n","    data = [{k: v for k, v in row.items()} for row in csv.DictReader(f, skipinitialspace=True)] \n","\n","random.shuffle(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jXssbDK-utdU"},"source":["train_data = data[:9000]\n","test_data = data[9000:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YJwM8t5mvZMg"},"source":["# Start to define the preprocess function and separate Label from text"]},{"cell_type":"code","metadata":{"id":"8J2yCP9cvLN6"},"source":["def clean_tweet(tweet):\n","    \n","    # tokenize tweet\n","    tokenized_tweet = \" \".join(tokenizer.tokenize(tweet))\n","    # remove unprintable characters (ð\\x9f\\x98\\x81ð\\x9f\\x98\\x81 for ex)\n","    printable_tweet = ''.join(filter(lambda x: x in set(string.printable), tokenized_tweet))\n","    \n","    # replace links (https://www. for ex) HTTPURL\n","    linkless_tweet = re.sub(r\"http\\S+\", \"HTTPURL\", printable_tweet)\n","    \n","    # remove retweets (RT)\n","    retweetless_tweet = re.sub(r\"RT\", \"\", linkless_tweet)\n","    \n","    # replace mentions (@user for example) with @USER\n","    mentionless_tweet = re.sub(r\"@[\\S]+\", \"@USER\", retweetless_tweet)\n","    \n","    # remove other odd stuff (&... for ex)\n","    other = re.sub(r\"&[\\S]+\", \"\", mentionless_tweet)\n","    \n","    # replace emojis with str\n","    emojiless_tweet = emoji.demojize(other)\n","    \n","    # remove unwanted whitespace (ex \"Hello        XWorld\" => \"hell world\")\n","    whitespaceless = re.sub(r'\\s+', ' ', emojiless_tweet)\n","    \n","    return whitespaceless"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IsupJWKPvWwp"},"source":["train_data_tweets = [clean_tweet(row[\"text\"]) for row in train_data] \n","train_data_labels = [int(row[\"label\"]) for row in train_data]\n","\n","test_data_tweets = [clean_tweet(row[\"text\"]) for row in test_data]\n","test_data_labels = [int(row[\"label\"]) for row in test_data]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gwIOEuVg2UoT"},"source":["train_d = {'text': [clean_tweet(row[\"text\"]) for row in train_data] , 'label': [int(row[\"label\"]) for row in train_data]}\n","test_d = {'text': [clean_tweet(row[\"text\"]) for row in test_data] , 'label': [int(row[\"label\"]) for row in test_data]}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y4sBfS103kpD"},"source":["# Save the clean datasets"]},{"cell_type":"code","metadata":{"id":"5OtHjLXtv0NI"},"source":["train_frame = pd.DataFrame(train_d)\n","test_frame = pd.DataFrame(test_d)\n","\n","train_frame.to_csv('clean_train_set.csv', header=True)\n","test_frame.to_csv('clean_test_set.csv', header=True)"],"execution_count":null,"outputs":[]}]}