{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"sexism-detection-roBERTa_model.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":321.193756,"end_time":"2021-05-07T11:28:02.160579","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-05-07T11:22:40.966823","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"27410b4f3ff644d6947968f8fd5e4785":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6e4bb85b60014869b38a6ee337b822f0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ec97a7c61076409c88a7814cc4926f12","IPY_MODEL_03e6cc8b0214440c8a4cc37f878ebbcb"]}},"6e4bb85b60014869b38a6ee337b822f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ec97a7c61076409c88a7814cc4926f12":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3a8d3853e9344440ad5035cb78afb910","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":565,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":565,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2f913d8f173e475db79616932504f138"}},"03e6cc8b0214440c8a4cc37f878ebbcb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e2dd0df544584fa2b41a8a45a9a68af8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 565/565 [00:00&lt;00:00, 2.28kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2b240da4632f4f2eadbcb8aebc5b36ea"}},"3a8d3853e9344440ad5035cb78afb910":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2f913d8f173e475db79616932504f138":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e2dd0df544584fa2b41a8a45a9a68af8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2b240da4632f4f2eadbcb8aebc5b36ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f0a129b8100d4f57852b2022c789ac2d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_de37a40e47104cdf897f2c6f93e3a3a2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3a59e33be7cc4778950ed8d4bb601af0","IPY_MODEL_675b9877e6ce481bb09012097fed86f5"]}},"de37a40e47104cdf897f2c6f93e3a3a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3a59e33be7cc4778950ed8d4bb601af0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_618c4dbd55574a93a123cb4aac67345a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":501204462,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":501204462,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7fc52bf269a948c8874312aa69b28440"}},"675b9877e6ce481bb09012097fed86f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2b3c2d2fa260457f8abf0ff01947acb5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 501M/501M [00:16&lt;00:00, 30.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a04d1af33d7849279f89c305049d9e66"}},"618c4dbd55574a93a123cb4aac67345a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7fc52bf269a948c8874312aa69b28440":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2b3c2d2fa260457f8abf0ff01947acb5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a04d1af33d7849279f89c305049d9e66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b7b11df54f65496d990a3bd9c612e932":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e9dc3cf1887e499ab4f85330224ed82e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9ee3f73909d6465fa50c2cc95e6e5706","IPY_MODEL_1b3deb7e0f674145b55115901701e9e8"]}},"e9dc3cf1887e499ab4f85330224ed82e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9ee3f73909d6465fa50c2cc95e6e5706":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_117ac0804b88458796d439d7bdfc46b9","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":898823,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":898823,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fe3d0953dd2b41589a8b53432834464c"}},"1b3deb7e0f674145b55115901701e9e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_48505cb68b064a5db82330cf67c9787f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 899k/899k [00:00&lt;00:00, 1.08MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f4156d68dad1470189cb810ee3297f92"}},"117ac0804b88458796d439d7bdfc46b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fe3d0953dd2b41589a8b53432834464c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"48505cb68b064a5db82330cf67c9787f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f4156d68dad1470189cb810ee3297f92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"829cbc9ae19b4ee8b0a7cf516f8e5254":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d35a2b5da8784c70b1c82750d7908d14","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8e499e6e9b2e486489c2d459b40b085a","IPY_MODEL_6aef64f9747c42b8b272676bc2423cf4"]}},"d35a2b5da8784c70b1c82750d7908d14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8e499e6e9b2e486489c2d459b40b085a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b9d57764226a4d5f833565c1c21a3977","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_11131e818db448308caf06e57da5435d"}},"6aef64f9747c42b8b272676bc2423cf4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_51d10f55888c4cbfb1e08b4d60bde6e1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456k/456k [00:01&lt;00:00, 345kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0479a19577db4ec1a7972d328390e0e7"}},"b9d57764226a4d5f833565c1c21a3977":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"11131e818db448308caf06e57da5435d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"51d10f55888c4cbfb1e08b4d60bde6e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0479a19577db4ec1a7972d328390e0e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"intensive-cemetery"},"source":["# Sexism Detection - Part II: Training a Model\n","\n","In this notebook we use the general hatespeech dataset we constructed in the previous notebook ([here](https://www.kaggle.com/jessedingley/hatespeech-detection-data)) to build a general hate speech model that predicts if a tweet conveys hate speech or not. For this we use BERT for sequence classification.\n"],"id":"intensive-cemetery"},{"cell_type":"markdown","metadata":{"id":"abstract-civilian"},"source":["# 0. Setup"],"id":"abstract-civilian"},{"cell_type":"markdown","metadata":{"id":"placed-basic"},"source":["### Imports"],"id":"placed-basic"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-prOaXIILV7x","executionInfo":{"status":"ok","timestamp":1621687363039,"user_tz":-180,"elapsed":5088,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}},"outputId":"43e73920-7962-4c35-c301-1ed401b030d9"},"source":["# install huggingface\n","!pip install transformers\n","!pip install emoji"],"id":"-prOaXIILV7x","execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"composite-peoples","executionInfo":{"status":"ok","timestamp":1621687363041,"user_tz":-180,"elapsed":7,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}}},"source":["# for gpu use, tensors etc...\n","import torch\n","\n","# import tokenizer, model for sequence classification, trainer and training arguments\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n","\n","# We need to create Datasets types to pass into the model\n","from torch.utils.data import Dataset, DataLoader\n","\n","# for opening csv\n","import csv\n","\n","# for computing metrics\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef\n","\n","# for saving model\n","import os\n","\n","import pandas as pd\n"],"id":"composite-peoples","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"copyrighted-lucas"},"source":["### Model and Tokenizer setup\n","We're using the `distilbert-base-uncased` variant of BERT which is smaller and more efficient than regular BERT."],"id":"copyrighted-lucas"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":318,"referenced_widgets":["27410b4f3ff644d6947968f8fd5e4785","6e4bb85b60014869b38a6ee337b822f0","ec97a7c61076409c88a7814cc4926f12","03e6cc8b0214440c8a4cc37f878ebbcb","3a8d3853e9344440ad5035cb78afb910","2f913d8f173e475db79616932504f138","e2dd0df544584fa2b41a8a45a9a68af8","2b240da4632f4f2eadbcb8aebc5b36ea","f0a129b8100d4f57852b2022c789ac2d","de37a40e47104cdf897f2c6f93e3a3a2","3a59e33be7cc4778950ed8d4bb601af0","675b9877e6ce481bb09012097fed86f5","618c4dbd55574a93a123cb4aac67345a","7fc52bf269a948c8874312aa69b28440","2b3c2d2fa260457f8abf0ff01947acb5","a04d1af33d7849279f89c305049d9e66","b7b11df54f65496d990a3bd9c612e932","e9dc3cf1887e499ab4f85330224ed82e","9ee3f73909d6465fa50c2cc95e6e5706","1b3deb7e0f674145b55115901701e9e8","117ac0804b88458796d439d7bdfc46b9","fe3d0953dd2b41589a8b53432834464c","48505cb68b064a5db82330cf67c9787f","f4156d68dad1470189cb810ee3297f92","829cbc9ae19b4ee8b0a7cf516f8e5254","d35a2b5da8784c70b1c82750d7908d14","8e499e6e9b2e486489c2d459b40b085a","6aef64f9747c42b8b272676bc2423cf4","b9d57764226a4d5f833565c1c21a3977","11131e818db448308caf06e57da5435d","51d10f55888c4cbfb1e08b4d60bde6e1","0479a19577db4ec1a7972d328390e0e7"]},"id":"forced-shadow","executionInfo":{"status":"ok","timestamp":1621687385061,"user_tz":-180,"elapsed":22025,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}},"outputId":"34174b70-5efb-4d64-93a5-6f78476df241"},"source":["MODEL_NAME = \"cardiffnlp/twitter-roberta-base\" # Distil BERT is a smaller model with faster execution time\n","\n","# define model and tokenizer\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2, output_attentions=False, output_hidden_states=False)\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side = \"right\")"],"id":"forced-shadow","execution_count":6,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27410b4f3ff644d6947968f8fd5e4785","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=565.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0a129b8100d4f57852b2022c789ac2d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501204462.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b7b11df54f65496d990a3bd9c612e932","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"829cbc9ae19b4ee8b0a7cf516f8e5254","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"revolutionary-celtic"},"source":["### GPU"],"id":"revolutionary-celtic"},{"cell_type":"code","metadata":{"id":"concerned-anger","executionInfo":{"status":"ok","timestamp":1621687385062,"user_tz":-180,"elapsed":16,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"id":"concerned-anger","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"metropolitan-wholesale"},"source":["Disable wandb"],"id":"metropolitan-wholesale"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"roman-above","executionInfo":{"status":"ok","timestamp":1621687385063,"user_tz":-180,"elapsed":15,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}},"outputId":"10d75a8e-57cc-4f75-8f2c-2ed53d9d60de"},"source":["%env WANDB_DISABLED=true"],"id":"roman-above","execution_count":8,"outputs":[{"output_type":"stream","text":["env: WANDB_DISABLED=true\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"flying-performance"},"source":["# 1. Data"],"id":"flying-performance"},{"cell_type":"markdown","metadata":{"id":"computational-cocktail"},"source":["## 1.1. Retrieve Train and Test Data"],"id":"computational-cocktail"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQcgnzBoLgpf","executionInfo":{"status":"ok","timestamp":1621687385401,"user_tz":-180,"elapsed":350,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}},"outputId":"8a7770c7-4291-450e-fd20-6b9e6bdb46e0"},"source":["!git clone https://github.com/MeliLuca/Natural_Language_Processing"],"id":"iQcgnzBoLgpf","execution_count":9,"outputs":[{"output_type":"stream","text":["Cloning into 'Natural_Language_Processing'...\n","remote: Enumerating objects: 10, done.\u001b[K\n","remote: Counting objects: 100% (10/10), done.\u001b[K\n","remote: Compressing objects: 100% (8/8), done.\u001b[K\n","remote: Total 10 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (10/10), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"abandoned-wisconsin","executionInfo":{"status":"ok","timestamp":1621687385402,"user_tz":-180,"elapsed":7,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}}},"source":["# open train data\n","with open(\"/content/Natural_Language_Processing/clean_train_set.csv\", \"r\", encoding=\"utf8\") as f:\n","    train_data = [{k: v for k, v in row.items()} for row in csv.DictReader(f, skipinitialspace=True)] \n","with open(\"/content/Natural_Language_Processing/clean_test_set.csv\", \"r\", encoding=\"utf8\") as f:\n","    test_data = [{k: v for k, v in row.items()} for row in csv.DictReader(f, skipinitialspace=True)] "],"id":"abandoned-wisconsin","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"disturbed-comparison"},"source":["## 1.2. Separate Tweets from Labels"],"id":"disturbed-comparison"},{"cell_type":"code","metadata":{"id":"deadly-agency","executionInfo":{"status":"ok","timestamp":1621687385402,"user_tz":-180,"elapsed":6,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}}},"source":["train_data_tweets = [row[\"text\"] for row in train_data] \n","train_data_labels = [int(row[\"label\"]) for row in train_data]\n","\n","test_data_tweets = [row[\"text\"] for row in test_data]\n","test_data_labels = [int(row[\"label\"]) for row in test_data]"],"id":"deadly-agency","execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"broad-company"},"source":["## 1.3. Split Training Data into Train and Validation splits"],"id":"broad-company"},{"cell_type":"code","metadata":{"id":"rising-metabolism","executionInfo":{"status":"ok","timestamp":1621687385403,"user_tz":-180,"elapsed":6,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","# calculate valiation split size (it needs to represent 15% of all data)\n","val_split_size = (0.15*(len(train_data)+len(test_data)))/len(train_data)\n","\n","# split\n","train_tweets, val_tweets, train_labels, val_labels = train_test_split(train_data_tweets, train_data_labels, test_size=val_split_size)"],"id":"rising-metabolism","execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"desperate-muscle"},"source":["## 1.4. Tokenize Data\n","More specifically tokenize tweets."],"id":"desperate-muscle"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sweet-employee","executionInfo":{"status":"ok","timestamp":1621687387481,"user_tz":-180,"elapsed":2084,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}},"outputId":"22c13d8d-e7a0-4ce1-8120-3226c98e93d8"},"source":["tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side = \"right\")\n","\n","tokenized_train_tweets = tokenizer(train_tweets, truncation=True, padding=True)\n","tokenized_val_tweets = tokenizer(val_tweets, truncation=True, padding=True)\n","tokenized_test_tweets = tokenizer(test_data_tweets, truncation=True, padding=True)"],"id":"sweet-employee","execution_count":13,"outputs":[{"output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"collectible-poverty"},"source":["## 1.5. Construct `Dataset` class\n","This is the necessary format for training."],"id":"collectible-poverty"},{"cell_type":"code","metadata":{"id":"driving-peace","executionInfo":{"status":"ok","timestamp":1621687387482,"user_tz":-180,"elapsed":15,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}}},"source":["class HateSpeechDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"],"id":"driving-peace","execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"opposed-insured","executionInfo":{"status":"ok","timestamp":1621687387482,"user_tz":-180,"elapsed":13,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}}},"source":["train_dataset = HateSpeechDataset(tokenized_train_tweets, train_labels)\n","val_dataset = HateSpeechDataset(tokenized_val_tweets, val_labels)\n","test_dataset = HateSpeechDataset(tokenized_test_tweets, test_data_labels)"],"id":"opposed-insured","execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"crucial-hawaiian"},"source":["# 2. Training"],"id":"crucial-hawaiian"},{"cell_type":"markdown","metadata":{"id":"considerable-owner"},"source":["## 2.1. Set various Parameters"],"id":"considerable-owner"},{"cell_type":"markdown","metadata":{"id":"proud-tyler"},"source":["### 2.1.1. Model Hyperparameters"],"id":"proud-tyler"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"parallel-newman","executionInfo":{"status":"ok","timestamp":1621687387483,"user_tz":-180,"elapsed":13,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}},"outputId":"51d06977-e36c-491d-f533-1ebe2ef0ab9b"},"source":["training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=30, \n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=64,\n","    load_best_model_at_end = True, \n","    learning_rate=0.001,\n","    warmup_steps=100,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=100\n",")"],"id":"parallel-newman","execution_count":16,"outputs":[{"output_type":"stream","text":["Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"general-thanks"},"source":["### 2.1.2. Evaluation Metrics"],"id":"general-thanks"},{"cell_type":"code","metadata":{"id":"common-links","executionInfo":{"status":"ok","timestamp":1621687387484,"user_tz":-180,"elapsed":12,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}}},"source":["# A function computing metrics based on model output\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }"],"id":"common-links","execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"artistic-performer"},"source":["## 2.2. Train the model"],"id":"artistic-performer"},{"cell_type":"markdown","metadata":{"id":"strange-attachment"},"source":["### 2.2.1 Set model to training mode"],"id":"strange-attachment"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"executed-helmet","executionInfo":{"status":"ok","timestamp":1621687387484,"user_tz":-180,"elapsed":12,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}},"outputId":"4cb2aa6c-b507-484a-e329-cf8e822a47e3"},"source":["model.train()"],"id":"executed-helmet","execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"certified-effect"},"source":["### 2.2.2. Define `Trainer` (training setup)"],"id":"certified-effect"},{"cell_type":"code","metadata":{"id":"consistent-border","executionInfo":{"status":"ok","timestamp":1621687399089,"user_tz":-180,"elapsed":11613,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}}},"source":["trainer = Trainer(\n","    model=model,                         # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    compute_metrics=compute_metrics,\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")"],"id":"consistent-border","execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rolled-asset"},"source":["### 2.2.3. Freeze BERT layers\n","(We only want to train the classifier head)"],"id":"rolled-asset"},{"cell_type":"code","metadata":{"id":"endangered-viking","executionInfo":{"status":"ok","timestamp":1621687399093,"user_tz":-180,"elapsed":6,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}}},"source":["for name, param in model.named_parameters():\n","    if 'classifier' not in name:\n","        param.requires_grad = False"],"id":"endangered-viking","execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"metallic-symphony"},"source":["### 2.2.4. Actually train the model"],"id":"metallic-symphony"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"loose-ballot","executionInfo":{"status":"ok","timestamp":1621688182949,"user_tz":-180,"elapsed":783861,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}},"outputId":"63d2e522-e9a0-4476-e5d0-9e5f98f3c2ba"},"source":["trainer.train()"],"id":"loose-ballot","execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 13:03, Epoch 30/30]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.499600</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.392200</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.384800</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.370200</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.362900</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.363200</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.369500</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.371900</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.345900</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.371900</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.343000</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.350400</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.345700</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.347700</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.341300</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.355500</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.342000</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.344100</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.352500</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.341800</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.355700</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.342700</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.344900</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.338600</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.330400</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.342400</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.329100</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.342100</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.343700</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.344700</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.347200</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.319300</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.320500</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.355300</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.320700</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.342600</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>0.324300</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.332700</td>\n","    </tr>\n","    <tr>\n","      <td>3900</td>\n","      <td>0.328600</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.337000</td>\n","    </tr>\n","    <tr>\n","      <td>4100</td>\n","      <td>0.321100</td>\n","    </tr>\n","    <tr>\n","      <td>4200</td>\n","      <td>0.333800</td>\n","    </tr>\n","    <tr>\n","      <td>4300</td>\n","      <td>0.328100</td>\n","    </tr>\n","    <tr>\n","      <td>4400</td>\n","      <td>0.329200</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.329800</td>\n","    </tr>\n","    <tr>\n","      <td>4600</td>\n","      <td>0.317700</td>\n","    </tr>\n","    <tr>\n","      <td>4700</td>\n","      <td>0.325100</td>\n","    </tr>\n","    <tr>\n","      <td>4800</td>\n","      <td>0.323800</td>\n","    </tr>\n","    <tr>\n","      <td>4900</td>\n","      <td>0.318600</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.323300</td>\n","    </tr>\n","    <tr>\n","      <td>5100</td>\n","      <td>0.333300</td>\n","    </tr>\n","    <tr>\n","      <td>5200</td>\n","      <td>0.318300</td>\n","    </tr>\n","    <tr>\n","      <td>5300</td>\n","      <td>0.330000</td>\n","    </tr>\n","    <tr>\n","      <td>5400</td>\n","      <td>0.319800</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.334400</td>\n","    </tr>\n","    <tr>\n","      <td>5600</td>\n","      <td>0.312500</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.305600</td>\n","    </tr>\n","    <tr>\n","      <td>5800</td>\n","      <td>0.319900</td>\n","    </tr>\n","    <tr>\n","      <td>5900</td>\n","      <td>0.329200</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.322900</td>\n","    </tr>\n","    <tr>\n","      <td>6100</td>\n","      <td>0.328400</td>\n","    </tr>\n","    <tr>\n","      <td>6200</td>\n","      <td>0.311800</td>\n","    </tr>\n","    <tr>\n","      <td>6300</td>\n","      <td>0.323700</td>\n","    </tr>\n","    <tr>\n","      <td>6400</td>\n","      <td>0.308200</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.323400</td>\n","    </tr>\n","    <tr>\n","      <td>6600</td>\n","      <td>0.300100</td>\n","    </tr>\n","    <tr>\n","      <td>6700</td>\n","      <td>0.316600</td>\n","    </tr>\n","    <tr>\n","      <td>6800</td>\n","      <td>0.329500</td>\n","    </tr>\n","    <tr>\n","      <td>6900</td>\n","      <td>0.325000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=6990, training_loss=0.3386640243093684, metrics={'train_runtime': 783.67, 'train_samples_per_second': 8.92, 'total_flos': 9341657226216000.0, 'epoch': 30.0, 'init_mem_cpu_alloc_delta': 2364698624, 'init_mem_gpu_alloc_delta': 499887104, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 14667776, 'train_mem_gpu_alloc_delta': 7118336, 'train_mem_cpu_peaked_delta': 0, 'train_mem_gpu_peaked_delta': 112434176})"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"excited-identification"},"source":["# 3. Evaluation"],"id":"excited-identification"},{"cell_type":"markdown","metadata":{"id":"colored-giant"},"source":["## 3.1. Evaluation on Dev Set"],"id":"colored-giant"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"romance-right","executionInfo":{"status":"ok","timestamp":1621688187806,"user_tz":-180,"elapsed":4871,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}},"outputId":"4c8999e8-8f63-4bd3-bb75-461294690660"},"source":["trainer.evaluate()"],"id":"romance-right","execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25/25 00:04]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["{'epoch': 30.0,\n"," 'eval_accuracy': 0.868370607028754,\n"," 'eval_f1': 0.7193460490463215,\n"," 'eval_loss': 0.32111942768096924,\n"," 'eval_mem_cpu_alloc_delta': 376832,\n"," 'eval_mem_cpu_peaked_delta': 0,\n"," 'eval_mem_gpu_alloc_delta': 0,\n"," 'eval_mem_gpu_peaked_delta': 106092032,\n"," 'eval_precision': 0.7880597014925373,\n"," 'eval_recall': 0.6616541353383458,\n"," 'eval_runtime': 4.618,\n"," 'eval_samples_per_second': 338.89}"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"constitutional-shell"},"source":["## 3.2. Evaluation on test set."],"id":"constitutional-shell"},{"cell_type":"markdown","metadata":{"id":"motivated-happiness"},"source":["### Set model to evaluate mode"],"id":"motivated-happiness"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"alternate-reynolds","executionInfo":{"status":"ok","timestamp":1621688187807,"user_tz":-180,"elapsed":12,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}},"outputId":"275a2d1a-cb00-474c-fe83-61454afde58a"},"source":["model.eval()"],"id":"alternate-reynolds","execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"static-winter"},"source":["### Function to predict output of a tweet"],"id":"static-winter"},{"cell_type":"code","metadata":{"id":"utility-jewelry","executionInfo":{"status":"ok","timestamp":1621688187807,"user_tz":-180,"elapsed":7,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}}},"source":["def get_sent_pred(input_str,device=device):\n","    tok = tokenizer(input_str, return_tensors=\"pt\", truncation=True, padding=True)\n","    tok.to(device)\n","    with torch.no_grad():\n","        pred = model(**tok)\n","    return pred['logits'].argmax(-1).item()"],"id":"utility-jewelry","execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"efficient-suggestion"},"source":["### Function to compute metrics of model output for test data"],"id":"efficient-suggestion"},{"cell_type":"code","metadata":{"id":"convertible-there","executionInfo":{"status":"ok","timestamp":1621688187808,"user_tz":-180,"elapsed":7,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}}},"source":["def compute_metrics_test(y_true,y_pred):\n","    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n","    acc = accuracy_score(y_true, y_pred)\n","    matthews = matthews_corrcoef(y_true, y_pred)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall,\n","        'matthews': matthews\n","}"],"id":"convertible-there","execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"virtual-context"},"source":["### Compute Metrics"],"id":"virtual-context"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hundred-medicine","executionInfo":{"status":"ok","timestamp":1621688201586,"user_tz":-180,"elapsed":13784,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}},"outputId":"d3016a82-9cdd-46cc-885c-50f91e595f47"},"source":["compute_metrics_test(test_data_labels, [get_sent_pred(sent) for sent in test_data_tweets])"],"id":"hundred-medicine","execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 0.880586592178771,\n"," 'f1': 0.7405159332321699,\n"," 'matthews': 0.6702518996035236,\n"," 'precision': 0.8271186440677966,\n"," 'recall': 0.6703296703296703}"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"entitled-electric"},"source":["# 4. Save model"],"id":"entitled-electric"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"consecutive-associate","executionInfo":{"status":"ok","timestamp":1621688654078,"user_tz":-180,"elapsed":1729,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}},"outputId":"ad59b376-eb34-4377-d5c2-a669e26827d3"},"source":["# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","output_dir = '/content/Natural_Language_Processing/model_save'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n"],"id":"consecutive-associate","execution_count":29,"outputs":[{"output_type":"stream","text":["Saving model to /content/Natural_Language_Processing/model_save\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["('/content/Natural_Language_Processing/model_save/tokenizer_config.json',\n"," '/content/Natural_Language_Processing/model_save/special_tokens_map.json',\n"," '/content/Natural_Language_Processing/model_save/vocab.json',\n"," '/content/Natural_Language_Processing/model_save/merges.txt',\n"," '/content/Natural_Language_Processing/model_save/added_tokens.json',\n"," '/content/Natural_Language_Processing/model_save/tokenizer.json')"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F0Vdt_NiLhjw","executionInfo":{"status":"ok","timestamp":1621689330479,"user_tz":-180,"elapsed":24908,"user":{"displayName":"Luca Melissari","photoUrl":"","userId":"15176372140984288016"}},"outputId":"58f31a55-22b1-48bb-c5eb-921de9e4b976"},"source":["!zip -r /content/file.zip /content/Natural_Language_Processing/model_save/\n"],"id":"F0Vdt_NiLhjw","execution_count":34,"outputs":[{"output_type":"stream","text":["  adding: content/Natural_Language_Processing/model_save/ (stored 0%)\n","  adding: content/Natural_Language_Processing/model_save/tokenizer.json (deflated 59%)\n","  adding: content/Natural_Language_Processing/model_save/special_tokens_map.json (deflated 83%)\n","  adding: content/Natural_Language_Processing/model_save/config.json (deflated 49%)\n","  adding: content/Natural_Language_Processing/model_save/tokenizer_config.json (deflated 79%)\n","  adding: content/Natural_Language_Processing/model_save/vocab.json (deflated 59%)\n","  adding: content/Natural_Language_Processing/model_save/merges.txt (deflated 53%)\n","  adding: content/Natural_Language_Processing/model_save/pytorch_model.bin (deflated 7%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dvUV6PoGNxjh"},"source":["from google.colab import files\n","files.download(\"/content/file.zip\")"],"id":"dvUV6PoGNxjh","execution_count":null,"outputs":[]}]}